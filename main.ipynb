{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8819d4b6-a920-457d-a33b-e0e9ec11acd7",
   "metadata": {},
   "source": [
    "# Main Narrative Notebook -- Predicting Income Off of General Census Data, a STAT 159 Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0ca905-1ce6-4ffe-a9e8-7f9e92134b9e",
   "metadata": {},
   "source": [
    "## EDA Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3b3353c-d0da-4127-92d6-ba6484165cc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aed0963-5034-4061-bb57-097764871ef0",
   "metadata": {},
   "source": [
    "## Feature Engineering Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee9a8e8-9296-43c4-9c6c-68b9d22c9d0c",
   "metadata": {},
   "source": [
    "Now let us go over Feature Engineering best practices that we have found across our various versions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c087801-886d-4f65-af69-bf0a719d1309",
   "metadata": {},
   "source": [
    "### Version 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08248fad-9448-4aac-950b-2c973fa593d6",
   "metadata": {},
   "source": [
    "Version 1 has a lot of standard, to-be-expected feature engineering, along with some interesting approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5602aff8-697f-4862-88d7-b3ea626b0191",
   "metadata": {},
   "source": [
    "##### Let us start with the standard stuff. \n",
    "\n",
    "There is a lot of one-hot encoding for the categorical variables included in the dataset. This lets us use numerical inputs for our model instead of the strings the categorical variables originally stored. \n",
    "\n",
    "Another fairly standard feature-engineering method used is log-transforms. These are especially useful for numerical variables where the later numbers need less weightage than the earlier numbers. This method was used on the following variables: age, years in education, capital-gain, capital-loss, and hours worked per week."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f14fef2-7672-45af-8c7f-428794375a28",
   "metadata": {},
   "source": [
    "##### Now, let us talk about some less standard, interesting methods used.\n",
    "\n",
    "Version 1 utilized combined features, meaning it combined different features through various methods. The purpose of this is that a feature may be stronger in terms of model performance if two correlated features are combined and used as a single feature within a model. These very well may have contributed to the solid performance of the version 1 model.\n",
    "\n",
    "The combined features used were: years educated / hours worked and capital gains * age. Years educated / hours worked is interesting, as higher years educated are correlated with less hours worked, so this correlation creates larger or higher values of the ratio. Additionally for capital gains * age, there is a positive relationship between the two, so multiplying amplifies the affect.\n",
    "\n",
    "It should be noted that the ratios utilized used the log versions of the original continuous variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b78c2ba-a8db-4dd3-b77c-dacee3e52f1e",
   "metadata": {},
   "source": [
    "##### There were some miscellaneous changes as well.\n",
    "\n",
    "The target variable was one-hot encoded to be <=50k as a binary variable. Additionally, all the variables that were one-hot encoded or used for log transforms were dropped, as they were not needed for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe3c5c9-cd96-49f3-a22f-4dbefa471d67",
   "metadata": {},
   "source": [
    "### Version 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7131cdb6-ea82-4ae6-8509-74b037b8d63d",
   "metadata": {},
   "source": [
    "Version 2 takes a simple/classical, but efficient approach to feature engineering. Sometimes doing less is more for feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f78aef-04e5-4cbc-a66d-aa7b2fa48197",
   "metadata": {},
   "source": [
    "##### Let us start with the standard stuff. \n",
    "\n",
    "Version 2 made the excellent observation that the dataset uses \"?\" instead of NA values, which is not obvious if checking for NA's via standard Pandas functions. Version 2 made the decision to drop these NA values, which contrasts the decision made in Version 3, where the choice to not drop these NA's was made. Regardless, there is no right or wrong in this scenario and dropping them was a solid decision.\n",
    "\n",
    "Next, all the appropriate categorical variables were one-hot encoded. Additionally, the original, unmodified versions of features were dropped from the dataset, as they have no use in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc95386-e3ed-4194-92de-830c24352192",
   "metadata": {},
   "source": [
    "##### Now, let us talk about some less standard, interesting methods used.\n",
    "\n",
    "One super interesting thing about Version 2 is that they noticed that the data in the target variable (income level) was imbalanced and chose to resample to account for this. This is a very unique method to use in feature engineering to account for an imbalance.\n",
    "\n",
    "Additionally, standardized data scaling was used for the continuous variables. This is an interesting way to weight the importance of continuous variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3db7048-49ec-4c29-8122-57341f279d41",
   "metadata": {},
   "source": [
    "### Version 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8bafae-5eb3-4081-8438-a7275e76757c",
   "metadata": {},
   "source": [
    "Version 3 is very methodical and goes in-depth into more standard feature engineering practices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aff3441-3d44-40ed-9e43-8079d8028d41",
   "metadata": {},
   "source": [
    "##### Let us start with the standard stuff. \n",
    "\n",
    "Version 3 made the excellent choice to check for non-standard NA coding. If one does a cursory check of the data with pandas functions, it would look like there are no NA values. However, Version 3 checked for NA values in different forms and it turns out the data uses \"?\" for its NA values. Version 3 made the decision to not impute these ? variables, as the difference is probably neglibile.\n",
    "\n",
    "Next, Version 3 makes the choice to cut out any redundant features within the dataset. The most prominent ones being the number of years spent in education and highest education, as they essentially convey the same information. The highest education variable ended up being dropped.\n",
    "\n",
    "Next, the target variable was encoded to become binary instead of being <=50k and >50k. Next, the categorical variables were one-hot encoded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede0969e-4845-4231-a468-773ec57afed5",
   "metadata": {},
   "source": [
    "##### Now, let us talk about some less standard, interesting methods used.\n",
    "\n",
    "The utilization of a fit_transform was interesting, where the data was first fit and then transformed. Another super interesting method used was a StandardScaler object, where a fit-transform was used on the continuous variables to standardize them. The choice to standardize them is interesting, as it is a safe way to transform continuous variables as opposed to doing log transforms, like in the version 1 example.\n",
    "\n",
    "Overall, this feature engineering process was made to be very clean and generalizable. It should perform well across many models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeeaccb9-3477-4ed4-8920-2de3f553d3f2",
   "metadata": {},
   "source": [
    "### Version 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b07d767-68c5-49f4-ae2f-bb24a8dcceb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fe3418-2f82-4821-9e18-a6d558acd331",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42f568f-466e-4dda-b352-d29483c1e4ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "181ba209-8b6b-41cd-9c0b-3e3e06267e60",
   "metadata": {},
   "source": [
    "### Best Feature Engineering Practices Learned Across Versions 1, 2, 3, and 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21afd6a6-e57a-452f-b347-88b489154c3a",
   "metadata": {},
   "source": [
    "Now let us discuss the best feature engineering practices found across Versions 1, 2, 3, and 4 and how they can be applied together in future approaches to this census income problem or even similar ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70138628-6e1b-4f92-a2c4-091d6c989367",
   "metadata": {},
   "source": [
    "1.\n",
    "\n",
    "The first thing we will be discussing is not strictly feature engineering, but it is an important thing to think about. Datasets are not always standardized. In this particular example, there are no NA values, yet there are still blank rows that use \"?\" instead of NA. These don't appear under standard Pandas functions, but are still important. In feature engineering, it is important to account for these non-standardized datasets. Whether we choose to impute or use other methods is another story, but looking for these little details can go a long away in our feature engineering efforts and eventual model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e65116-fbc2-4174-8c51-1eb16e1a7801",
   "metadata": {},
   "source": [
    "2.\n",
    "\n",
    "One-hot encoding is a fairly obvious feature engineering effort, but is important nonetheless. Making sure to \"numerify\" categorical variables is super important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccaec9f-b4c4-4440-b003-74adf6734432",
   "metadata": {},
   "source": [
    "3.\n",
    "\n",
    "Cutting redundancy is important in feature engineering. If two features essentially communicate the same thing, removing one will help improve efficiency of the model and prevent the overweighting of a feature genre."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f346a573-f51b-4b4a-99fc-c89541022900",
   "metadata": {},
   "source": [
    "4.\n",
    "\n",
    "Standardizing the features is another interesting method. It helps to reduce the range of the values and weights values different based on how close they are to the center. This could give extrema less of an impact on the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea429ea-c13d-4b38-95b5-6d56cc516bf1",
   "metadata": {},
   "source": [
    "5.\n",
    "\n",
    "In a similar vein to number 4, applying log transforms to features is a great practice in feature engineering. Some features, such as age, have diminishing returns as the values get higher. In these situations, applying a log-transform to make jumps in lower values to be weighted more can be helpful and change the way extrema are viewed by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d6f8f2-b0ce-4a7c-b46b-69bcbba3ed8d",
   "metadata": {},
   "source": [
    "6.\n",
    "\n",
    "Another great practice is combine features if they have some sort of correlation. For example, in version 1 there was a ratio of years educated / hours worked. These two features have a high correlation and combining two such features can potentially make models perform better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da65a2ea-c1cd-4122-b93a-cd62b5d1fece",
   "metadata": {},
   "source": [
    "## Modeling/Testing Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07c93014-236c-43ee-8239-75bd43dcee7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ddfa04-98c5-4021-83a2-9044938bc242",
   "metadata": {},
   "source": [
    "## Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e903c4a3-2a4e-47fb-9650-8e6ba85f0ca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa140b43-0a50-4c19-befd-8f39e401ac20",
   "metadata": {},
   "source": [
    "## Author Contributions Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfeb6bd-3b66-4bd8-8dec-479804033d20",
   "metadata": {},
   "source": [
    "Kavin:\n",
    "\n",
    "* Version 1 -- EDA.ipynb, FeatureEngineering.ipynb, Modeling.ipynb\n",
    "* Makefile\n",
    "* README.md\n",
    "* Feature Engineering Analysis -- main.ipynb\n",
    "* Repository Structuring\n",
    "* Initializing + Formatting Notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83b56fb-20c9-46e5-94a0-4f0efaf3a709",
   "metadata": {},
   "source": [
    "George McIntire:\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8f0b5c-3847-4e28-9452-0b0526fd39a6",
   "metadata": {},
   "source": [
    "Wen-Ching (Naomi) Tu:\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177e4153-cd76-4c24-8926-c9ec3e3f6ff7",
   "metadata": {},
   "source": [
    "Winston Cai:\n",
    "\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
